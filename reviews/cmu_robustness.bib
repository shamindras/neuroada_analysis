
@article{chen_general_2015,
	title = {A {General} {Decision} {Theory} for {Huber}'s \${\textbackslash}epsilon\$-{Contamination} {Model}},
	url = {http://arxiv.org/abs/1511.04144},
	abstract = {Today's data pose unprecedented challenges to statisticians. It may be incomplete, corrupted or exposed to some unknown source of contamination. We need new methods and theories to grapple with these challenges. Robust estimation is one of the revived fields with potential to accommodate such complexity and glean useful information from modern datasets. Following our recent work on high dimensional robust covariance matrix estimation, we establish a general decision theory for robust statistics under Huber's \${\textbackslash}epsilon\$-contamination model. We propose a solution using Scheff\{{\textbackslash}'e\} estimate to a robust two-point testing problem that leads to the construction of robust estimators adaptive to the proportion of contamination. Applying the general theory, we construct robust estimators for nonparametric density estimation, sparse linear regression and low-rank trace regression. We show that these new estimators achieve the minimax rate with optimal dependence on the contamination proportion. This testing procedure, Scheff\{{\textbackslash}'e\} estimate, also enjoys an optimal rate in the exponent of the testing error, which may be of independent interest.},
	urldate = {2017-12-23},
	journal = {arXiv:1511.04144 [math, stat]},
	author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.04144},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv\:1511.04144 PDF:files/4/Chen et al. - 2015 - A General Decision Theory for Huber's \$epsilon\$-C.pdf:application/pdf;arXiv.org Snapshot:files/5/1511.html:text/html}
}

@article{chen_robust_2015,
	title = {Robust {Covariance} and {Scatter} {Matrix} {Estimation} under {Huber}'s {Contamination} {Model}},
	url = {http://arxiv.org/abs/1506.00691},
	abstract = {Covariance matrix estimation is one of the most important problems in statistics. To accommodate the complexity of modern datasets, it is desired to have estimation procedures that not only can incorporate the structural assumptions of covariance matrices, but are also robust to outliers from arbitrary sources. In this paper, we define a new concept called matrix depth and then propose a robust covariance matrix estimator by maximizing the empirical depth function. The proposed estimator is shown to achieve minimax optimal rate under Huber's \${\textbackslash}epsilon\$-contamination model for estimating covariance/scatter matrices with various structures including bandedness and sparsity.},
	urldate = {2017-12-23},
	journal = {arXiv:1506.00691 [math, stat]},
	author = {Chen, Mengjie and Gao, Chao and Ren, Zhao},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.00691},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	file = {arXiv\:1506.00691 PDF:files/7/Chen et al. - 2015 - Robust Covariance and Scatter Matrix Estimation un.pdf:application/pdf;arXiv.org Snapshot:files/8/1506.html:text/html}
}

@article{lai_agnostic_2016,
	title = {Agnostic {Estimation} of {Mean} and {Covariance}},
	url = {http://arxiv.org/abs/1604.06968},
	abstract = {We consider the problem of estimating the mean and covariance of a distribution from iid samples in \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$, in the presence of an \${\textbackslash}eta\$ fraction of malicious noise; this is in contrast to much recent work where the noise itself is assumed to be from a distribution of known type. The agnostic problem includes many interesting special cases, e.g., learning the parameters of a single Gaussian (or finding the best-fit Gaussian) when \${\textbackslash}eta\$ fraction of data is adversarially corrupted, agnostically learning a mixture of Gaussians, agnostic ICA, etc. We present polynomial-time algorithms to estimate the mean and covariance with error guarantees in terms of information-theoretic lower bounds. As a corollary, we also obtain an agnostic algorithm for Singular Value Decomposition.},
	urldate = {2017-12-23},
	journal = {arXiv:1604.06968 [cs, stat]},
	author = {Lai, Kevin A. and Rao, Anup B. and Vempala, Santosh},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06968},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1604.06968 PDF:files/13/Lai et al. - 2016 - Agnostic Estimation of Mean and Covariance.pdf:application/pdf;arXiv.org Snapshot:files/14/1604.html:text/html}
}

@article{diakonikolas_robust_2016,
	title = {Robust {Estimators} in {High} {Dimensions} without the {Computational} {Intractability}},
	url = {http://arxiv.org/abs/1604.06443},
	abstract = {We study high-dimensional distribution learning in an agnostic setting where an adversary is allowed to arbitrarily corrupt an \${\textbackslash}varepsilon\$-fraction of the samples. Such questions have a rich history spanning statistics, machine learning and theoretical computer science. Even in the most basic settings, the only known approaches are either computationally inefficient or lose dimension-dependent factors in their error guarantees. This raises the following question:Is high-dimensional agnostic distribution learning even possible, algorithmically? In this work, we obtain the first computationally efficient algorithms with dimension-independent error guarantees for agnostically learning several fundamental classes of high-dimensional distributions: (1) a single Gaussian, (2) a product distribution on the hypercube, (3) mixtures of two product distributions (under a natural balancedness condition), and (4) mixtures of spherical Gaussians. Our algorithms achieve error that is independent of the dimension, and in many cases scales nearly-linearly with the fraction of adversarially corrupted samples. Moreover, we develop a general recipe for detecting and correcting corruptions in high-dimensions, that may be applicable to many other problems.},
	urldate = {2017-12-23},
	journal = {arXiv:1604.06443 [cs, math, stat]},
	author = {Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06443},
	keywords = {Mathematics - Statistics Theory, Computer Science - Data Structures and Algorithms, Computer Science - Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv\:1604.06443 PDF:files/16/Diakonikolas et al. - 2016 - Robust Estimators in High Dimensions without the C.pdf:application/pdf;arXiv.org Snapshot:files/19/1604.html:text/html}
}

@article{moitra_how_2015,
	title = {How {Robust} are {Reconstruction} {Thresholds} for {Community} {Detection}?},
	url = {http://arxiv.org/abs/1511.01473},
	abstract = {The stochastic block model is one of the oldest and most ubiquitous models for studying clustering and community detection. In an exciting sequence of developments, motivated by deep but non-rigorous ideas from statistical physics, Decelle et al. conjectured a sharp threshold for when community detection is possible in the sparse regime. Mossel, Neeman and Sly and Massoulie proved the conjecture and gave matching algorithms and lower bounds. Here we revisit the stochastic block model from the perspective of semirandom models where we allow an adversary to make `helpful' changes that strengthen ties within each community and break ties between them. We show a surprising result that these `helpful' changes can shift the information-theoretic threshold, making the community detection problem strictly harder. We complement this by showing that an algorithm based on semidefinite programming (which was known to get close to the threshold) continues to work in the semirandom model (even for partial recovery). This suggests that algorithms based on semidefinite programming are robust in ways that any algorithm meeting the information-theoretic threshold cannot be. These results point to an interesting new direction: Can we find robust, semirandom analogues to some of the classical, average-case thresholds in statistics? We also explore this question in the broadcast tree model, and we show that the viewpoint of semirandom models can help explain why some algorithms are preferred to others in practice, in spite of the gaps in their statistical performance on random models.},
	urldate = {2017-12-23},
	journal = {arXiv:1511.01473 [cs, math, stat]},
	author = {Moitra, Ankur and Perry, William and Wein, Alexander S.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.01473},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning, Statistics - Machine Learning, Computer Science - Information Theory, Mathematics - Probability},
	annote = {Comment: 36 pages, 3 figures},
	file = {arXiv\:1511.01473 PDF:files/20/Moitra et al. - 2015 - How Robust are Reconstruction Thresholds for Commu.pdf:application/pdf;arXiv.org Snapshot:files/23/1511.html:text/html}
}

@article{makarychev_learning_2015,
	title = {Learning {Communities} in the {Presence} of {Errors}},
	url = {http://arxiv.org/abs/1511.03229},
	abstract = {We study the problem of learning communities in the presence of modeling errors and give robust recovery algorithms for the Stochastic Block Model (SBM). This model, which is also known as the Planted Partition Model, is widely used for community detection and graph partitioning in various fields, including machine learning, statistics, and social sciences. Many algorithms exist for learning communities in the Stochastic Block Model, but they do not work well in the presence of errors. In this paper, we initiate the study of robust algorithms for partial recovery in SBM with modeling errors or noise. We consider graphs generated according to the Stochastic Block Model and then modified by an adversary. We allow two types of adversarial errors, Feige---Kilian or monotone errors, and edge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question about whether an almost exact recovery is possible when the adversary is allowed to add \$o(n)\$ edges. Our work answers this question affirmatively even in the case of \$k{\textgreater}2\$ communities. We then show that our algorithms work not only when the instances come from SBM, but also work when the instances come from any distribution of graphs that is \${\textbackslash}epsilon m\$ close to SBM in the Kullback---Leibler divergence. This result also works in the presence of adversarial errors. Finally, we present almost tight lower bounds for two communities.},
	urldate = {2017-12-23},
	journal = {arXiv:1511.03229 [cs, math, stat]},
	author = {Makarychev, Konstantin and Makarychev, Yury and Vijayaraghavan, Aravindan},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.03229},
	keywords = {Mathematics - Statistics Theory, Computer Science - Data Structures and Algorithms, Computer Science - Learning},
	annote = {Comment: 34 pages. Appearing in the Conference on Learning Theory (COLT)'16},
	file = {arXiv\:1511.03229 PDF:files/24/Makarychev et al. - 2015 - Learning Communities in the Presence of Errors.pdf:application/pdf;arXiv.org Snapshot:files/25/1511.html:text/html}
}

@article{du_computationally_2017,
	title = {Computationally {Efficient} {Robust} {Estimation} of {Sparse} {Functionals}},
	url = {http://arxiv.org/abs/1702.07709},
	abstract = {Many conventional statistical procedures are extremely sensitive to seemingly minor deviations from modeling assumptions. This problem is exacerbated in modern high-dimensional settings, where the problem dimension can grow with and possibly exceed the sample size. We consider the problem of robust estimation of sparse functionals, and provide a computationally and statistically efficient algorithm in the high-dimensional setting. Our theory identifies a unified set of deterministic conditions under which our algorithm guarantees accurate recovery. By further establishing that these deterministic conditions hold with high-probability for a wide range of statistical models, our theory applies to many problems of considerable interest including sparse mean and covariance estimation; sparse linear regression; and sparse generalized linear models.},
	urldate = {2017-12-23},
	journal = {arXiv:1702.07709 [cs, stat]},
	author = {Du, Simon S. and Balakrishnan, Sivaraman and Singh, Aarti},
	month = feb,
	year = {2017},
	note = {arXiv: 1702.07709},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1702.07709 PDF:files/27/Du et al. - 2017 - Computationally Efficient Robust Estimation of Spa.pdf:application/pdf;arXiv.org Snapshot:files/28/1702.html:text/html}
}

@article{kothari_better_2017,
	title = {Better {Agnostic} {Clustering} {Via} {Relaxed} {Tensor} {Norms}},
	url = {https://arxiv.org/abs/1711.07465},
	urldate = {2017-12-23},
	author = {Kothari, Pravesh K. and Steinhardt, Jacob},
	month = nov,
	year = {2017},
	file = {Full Text PDF:files/30/Kothari and Steinhardt - 2017 - Better Agnostic Clustering Via Relaxed Tensor Norm.pdf:application/pdf;Snapshot:files/31/1711.html:text/html}
}

@article{steinhardt_certified_2017,
	title = {Certified {Defenses} for {Data} {Poisoning} {Attacks}},
	url = {https://arxiv.org/abs/1706.03691},
	urldate = {2017-12-23},
	author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy},
	month = jun,
	year = {2017},
	file = {Full Text PDF:files/33/Steinhardt et al. - 2017 - Certified Defenses for Data Poisoning Attacks.pdf:application/pdf;Snapshot:files/34/1706.html:text/html}
}

@article{steinhardt_does_2017,
	title = {Does robustness imply tractability? {A} lower bound for planted clique in the semi-random model},
	shorttitle = {Does robustness imply tractability?},
	url = {http://arxiv.org/abs/1704.05120},
	abstract = {We consider a robust analog of the planted clique problem. In this analog, a set \$S\$ of vertices is chosen and all edges in \$S\$ are included; then, edges between \$S\$ and the rest of the graph are included with probability \${\textbackslash}frac\{1\}\{2\}\$, while edges not touching \$S\$ are allowed to vary arbitrarily. For this semi-random model, we show that the information-theoretic threshold for recovery is \${\textbackslash}tilde\{{\textbackslash}Theta\}({\textbackslash}sqrt\{n\})\$, in sharp contrast to the classical information-theoretic threshold of \${\textbackslash}Theta({\textbackslash}log(n))\$. This matches the conjectured computational threshold for the classical planted clique problem, and thus raises the intriguing possibility that, once we require robustness, there is no computational-statistical gap for planted clique.},
	urldate = {2017-12-23},
	journal = {arXiv:1704.05120 [cs, math, stat]},
	author = {Steinhardt, Jacob},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.05120},
	keywords = {Mathematics - Statistics Theory, Computer Science - Learning, Computer Science - Information Theory, Computer Science - Computational Complexity},
	file = {arXiv\:1704.05120 PDF:files/36/Steinhardt - 2017 - Does robustness imply tractability A lower bound .pdf:application/pdf;arXiv.org Snapshot:files/37/1704.html:text/html}
}

@article{steinhardt_resilience:_2017,
	title = {Resilience: {A} {Criterion} for {Learning} in the {Presence} of {Arbitrary} {Outliers}},
	shorttitle = {Resilience},
	url = {http://arxiv.org/abs/1703.04940},
	abstract = {We introduce a criterion, resilience, which allows properties of a dataset (such as its mean or best low rank approximation) to be robustly computed, even in the presence of a large fraction of arbitrary additional data. Resilience is a weaker condition than most other properties considered so far in the literature, and yet enables robust estimation in a broader variety of settings. We provide new information-theoretic results on robust distribution learning, robust estimation of stochastic block models, and robust mean estimation under bounded \$k\$th moments. We also provide new algorithmic results on robust distribution learning, as well as robust mean estimation in \${\textbackslash}ell\_p\$-norms. Among our proof techniques is a method for pruning a high-dimensional distribution with bounded \$1\$st moments to a stable "core" with bounded \$2\$nd moments, which may be of independent interest.},
	urldate = {2017-12-23},
	journal = {arXiv:1703.04940 [cs, stat]},
	author = {Steinhardt, Jacob and Charikar, Moses and Valiant, Gregory},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.04940},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Computational Complexity, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	annote = {Comment: 32 pages, full version of ITCS2018 paper (minor citation edit from v2)},
	file = {arXiv\:1703.04940 PDF:files/40/Steinhardt et al. - 2017 - Resilience A Criterion for Learning in the Presen.pdf:application/pdf;arXiv.org Snapshot:files/41/1703.html:text/html}
}

@article{charikar_learning_2016,
	title = {Learning from {Untrusted} {Data}},
	url = {http://arxiv.org/abs/1611.02315},
	abstract = {The vast majority of theoretical results in machine learning and statistics assume that the available training data is a reasonably reliable reflection of the phenomena to be learned or estimated. Similarly, the majority of machine learning and statistical techniques used in practice are brittle to the presence of large amounts of biased or malicious data. In this work we consider two frameworks in which to study estimation, learning, and optimization in the presence of significant fractions of arbitrary data. The first framework, list-decodable learning, asks whether it is possible to return a list of answers, with the guarantee that at least one of them is accurate. For example, given a dataset of \$n\$ points for which an unknown subset of \${\textbackslash}alpha n\$ points are drawn from a distribution of interest, and no assumptions are made about the remaining \$(1-{\textbackslash}alpha)n\$ points, is it possible to return a list of \${\textbackslash}operatorname\{poly\}(1/{\textbackslash}alpha)\$ answers, one of which is correct? The second framework, which we term the semi-verified learning model, considers the extent to which a small dataset of trusted data (drawn from the distribution in question) can be leveraged to enable the accurate extraction of information from a much larger but untrusted dataset (of which only an \${\textbackslash}alpha\$-fraction is drawn from the distribution). We show strong positive results in both settings, and provide an algorithm for robust learning in a very general stochastic optimization setting. This general result has immediate implications for robust estimation in a number of settings, including for robustly estimating the mean of distributions with bounded second moments, robustly learning mixtures of such distributions, and robustly finding planted partitions in random graphs in which significant portions of the graph have been perturbed by an adversary.},
	urldate = {2017-12-23},
	journal = {arXiv:1611.02315 [cs, math, stat]},
	author = {Charikar, Moses and Steinhardt, Jacob and Valiant, Gregory},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.02315},
	keywords = {Mathematics - Statistics Theory, Computer Science - Learning, Computer Science - Computational Complexity, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	annote = {Comment: Updated based on STOC camera-ready},
	file = {arXiv\:1611.02315 PDF:files/44/Charikar et al. - 2016 - Learning from Untrusted Data.pdf:application/pdf;arXiv.org Snapshot:files/45/1611.html:text/html}
}

@article{donoho_automatic_1988,
	title = {The "{Automatic}" {Robustness} of {Minimum} {Distance} {Functionals}},
	volume = {16},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176350820},
	doi = {10.1214/aos/1176350820},
	abstract = {The minimum distance (MD) functional defined by a distance μμ{\textbackslash}mu is automatically robust over contamination neighborhoods defined by μμ{\textbackslash}mu. In fact, when compared to other Fisher-consistent functionals, the MD functional was no worse than twice the minimum sensitivity to μμ{\textbackslash}mu-contamination, and at least half the best possible breakdown point. In invariant settings, the MD functional has the best attainable breakdown point against μμ{\textbackslash}mu-contamination among equivariant functionals. If μμ{\textbackslash}mu is Hilbertian (e.g., the Hellinger distance), the MD functional has the smallest sensitivity to μμ{\textbackslash}mu-contamination among Fisher-consistent functionals. The robustness of the MD functional is inherited by MD estimates, both estimates based on "weak" distances and estimates based on "strong" distances, when the empirical distribution is appropriately smoothed. These facts are general and apply not just in simple location models, but also in multivariate location-scatter and in semiparametric settings. Of course, this robustness is formal because μμ{\textbackslash}mu-contamination neighborhoods may not be large enough to contain realistic departures from the model. For the metrics we are interested in, robustness against μμ{\textbackslash}mu-contamination is stronger than robustness against gross errors contamination; and for "weak" metrics (e.g., μ=Cramer-von Mises, Kolmogorov)μ=Cramer-von Mises, Kolmogorov){\textbackslash}mu = {\textbackslash}text\{Cramer-von Mises, Kolmogorov\}), robustness over μμ{\textbackslash}mu-neighborhoods implies robustness over Prohorov neighborhoods.},
	language = {EN},
	number = {2},
	urldate = {2017-12-23},
	journal = {Ann. Statist.},
	author = {Donoho, David L. and Liu, Richard C.},
	month = jun,
	year = {1988},
	mrnumber = {MR947562},
	zmnumber = {0684.62030},
	keywords = {breakdown point, Cramer-von Mises discrepancy, gross-error sensitivity, Hellinger distances, Kolmogorov, Levy, Prohorov, Quantitative robustness, variation},
	pages = {552--586},
	file = {euclid.aos.1176350820.pdf:files/50/euclid.aos.1176350820.pdf:application/pdf;Snapshot:files/49/1176350820.html:text/html}
}

@article{yatracos_rates_1985,
	title = {Rates of {Convergence} of {Minimum} {Distance} {Estimators} and {Kolmogorov}'s {Entropy}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176349553},
	doi = {10.1214/aos/1176349553},
	abstract = {Let (𝒳,𝒜)(X,A)({\textbackslash}mathscr\{X, A\}) be a space with a σσ{\textbackslash}sigma-field, M=\{Ps;s∈Θ\}M=\{Ps;s∈Θ\}M = {\textbackslash}\{P\_s; s {\textbackslash}in {\textbackslash}Theta{\textbackslash}\} be a family of probability measures on 𝒜A{\textbackslash}mathscr\{A\} with ΘΘ{\textbackslash}Theta arbitrary, X1,⋯,XnX1,⋯,XnX\_1, {\textbackslash}cdots, X\_n i.i.d. observations on Pθ.Pθ.P\_{\textbackslash}theta. Define μn(A)=(1/n)∑ni=1IA(Xi),μn(A)=(1/n)∑i=1nIA(Xi),{\textbackslash}mu\_n(A) = (1/n) {\textbackslash}sum{\textasciicircum}n\_\{i = 1\} I\_A(X\_i), the empirical measure indexed by A∈𝒜.A∈A.A {\textbackslash}in {\textbackslash}mathscr\{A\}. Assume ΘΘ{\textbackslash}Theta is totally bounded when metrized by the L1L1L\_1 distance between measures. Robust minimum distance estimators θ̂ nθ{\textasciicircum}n{\textbackslash}hat\{{\textbackslash}theta\}\_n are constructed for θθ{\textbackslash}theta and the resulting rate of convergence is shown naturally to depend on an entropy function for ΘΘ{\textbackslash}Theta.},
	language = {EN},
	number = {2},
	urldate = {2017-12-23},
	journal = {Ann. Statist.},
	author = {Yatracos, Yannis G.},
	month = jun,
	year = {1985},
	mrnumber = {MR790571},
	zmnumber = {0576.62057},
	keywords = {density estimation, Kolmogorov's entropy, Minimum distance estimation, rates of convergence},
	pages = {768--774},
	file = {euclid.aos.1176349553.pdf:files/53/euclid.aos.1176349553.pdf:application/pdf;Snapshot:files/52/1176349553.html:text/html}
}

@article{nevo_bayesian_2015,
	title = {On {Bayesian} robust regression with diverging number of predictors},
	url = {http://arxiv.org/abs/1507.02074},
	abstract = {This paper concerns the robust regression model when the number of predictors and the number of observations grow in a similar rate. Theory for M-estimators in this regime has been recently developed by several authors [El Karoui et al., 2013, Bean et al., 2013, Donoho and Montanari, 2013]. Motivated by the inability of M-estimators to successfully estimate the Euclidean norm of the coefficient vector, we consider a Bayesian framework for this model. We suggest a two-component mixture of normals prior for the coefficients and develop a Gibbs sampler procedure for sampling from relevant posterior distributions, while utilizing a scale mixture of normal representation for the error distribution . Unlike M-estimators, the proposed Bayes estimator is consistent in the Euclidean norm sense. Simulation results demonstrate the superiority of the Bayes estimator over traditional estimation methods.},
	urldate = {2017-12-23},
	journal = {arXiv:1507.02074 [math, stat]},
	author = {Nevo, Daniel and Ritov, Ya'acov},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.02074},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	annote = {Comment: 18 pages},
	file = {arXiv\:1507.02074 PDF:files/56/Nevo and Ritov - 2015 - On Bayesian robust regression with diverging numbe.pdf:application/pdf;arXiv.org Snapshot:files/57/1507.html:text/html}
}

@article{donoho_high_2013,
	title = {High {Dimensional} {Robust} {M}-{Estimation}: {Asymptotic} {Variance} via {Approximate} {Message} {Passing}},
	shorttitle = {High {Dimensional} {Robust} {M}-{Estimation}},
	url = {http://arxiv.org/abs/1310.7320},
	abstract = {In a recent article (Proc. Natl. Acad. Sci., 110(36), 14557-14562), El Karoui et al. study the distribution of robust regression estimators in the regime in which the number of parameters p is of the same order as the number of samples n. Using numerical simulations and `highly plausible' heuristic arguments, they unveil a striking new phenomenon. Namely, the regression coefficients contain an extra Gaussian noise component that is not explained by classical concepts such as the Fisher information matrix. We show here that that this phenomenon can be characterized rigorously techniques that were developed by the authors to analyze the Lasso estimator under high-dimensional asymptotics. We introduce an approximate message passing (AMP) algorithm to compute M-estimators and deploy state evolution to evaluate the operating characteristics of AMP and so also M-estimates. Our analysis clarifies that the `extra Gaussian noise' encountered in this problem is fundamentally similar to phenomena already studied for regularized least squares in the setting n{\textless}p.},
	urldate = {2017-12-23},
	journal = {arXiv:1310.7320 [cs, math, stat]},
	author = {Donoho, David and Montanari, Andrea},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.7320},
	keywords = {Mathematics - Statistics Theory, Computer Science - Information Theory},
	annote = {Comment: 32 pages, 5 figures (v2 contains numerical simulations)},
	file = {arXiv\:1310.7320 PDF:files/60/Donoho and Montanari - 2013 - High Dimensional Robust M-Estimation Asymptotic V.pdf:application/pdf;arXiv.org Snapshot:files/61/1310.html:text/html}
}

@article{loh_statistical_2015,
	title = {Statistical consistency and asymptotic normality for high-dimensional robust {M}-estimators},
	url = {http://arxiv.org/abs/1501.00312},
	abstract = {We study theoretical properties of regularized robust M-estimators, applicable when data are drawn from a sparse high-dimensional linear model and contaminated by heavy-tailed distributions and/or outliers in the additive errors and covariates. We first establish a form of local statistical consistency for the penalized regression estimators under fairly mild conditions on the error distribution: When the derivative of the loss function is bounded and satisfies a local restricted curvature condition, all stationary points within a constant radius of the true regression vector converge at the minimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriate nonconvex regularizer is used in place of an l\_1-penalty, we show that such stationary points are in fact unique and equal to the local oracle solution with the correct support---hence, results on asymptotic normality in the low-dimensional case carry over immediately to the high-dimensional setting. This has important implications for the efficiency of regularized nonconvex M-estimators when the errors are heavy-tailed. Our analysis of the local curvature of the loss function also has useful consequences for optimization when the robust regression function and/or regularizer is nonconvex and the objective function possesses stationary points outside the local region. We show that as long as a composite gradient descent algorithm is initialized within a constant radius of the true regression vector, successive iterates will converge at a linear rate to a stationary point within the local region. Furthermore, the global optimum of a convex regularized robust regression function may be used to obtain a suitable initialization. The result is a novel two-step procedure that uses a convex M-estimator to achieve consistency and a nonconvex M-estimator to increase efficiency.},
	urldate = {2017-12-23},
	journal = {arXiv:1501.00312 [cs, math, stat]},
	author = {Loh, Po-Ling},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.00312},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Computer Science - Information Theory, 62F12},
	annote = {Comment: 56 pages, 8 figures},
	file = {arXiv\:1501.00312 PDF:files/64/Loh - 2015 - Statistical consistency and asymptotic normality f.pdf:application/pdf;arXiv.org Snapshot:files/65/1501.html:text/html}
}

@article{candes_robust_2009,
	title = {Robust {Principal} {Component} {Analysis}?},
	url = {http://arxiv.org/abs/0912.3599},
	abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
	urldate = {2017-12-23},
	journal = {arXiv:0912.3599 [cs, math]},
	author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
	month = dec,
	year = {2009},
	note = {arXiv: 0912.3599},
	keywords = {Computer Science - Information Theory},
	file = {arXiv\:0912.3599 PDF:files/68/Candes et al. - 2009 - Robust Principal Component Analysis.pdf:application/pdf;arXiv.org Snapshot:files/69/0912.html:text/html}
}

@article{mccoy_two_2011,
	title = {Two {Proposals} for {Robust} {PCA} using {Semidefinite} {Programming}},
	volume = {5},
	issn = {1935-7524},
	url = {http://arxiv.org/abs/1012.1086},
	doi = {10.1214/11-EJS636},
	abstract = {The performance of principal component analysis (PCA) suffers badly in the presence of outliers. This paper proposes two novel approaches for robust PCA based on semidefinite programming. The first method, maximum mean absolute deviation rounding (MDR), seeks directions of large spread in the data while damping the effect of outliers. The second method produces a low-leverage decomposition (LLD) of the data that attempts to form a low-rank model for the data by separating out corrupted observations. This paper also presents efficient computational methods for solving these SDPs. Numerical experiments confirm the value of these new techniques.},
	number = {0},
	urldate = {2017-12-23},
	journal = {Electronic Journal of Statistics},
	author = {McCoy, Michael and Tropp, Joel},
	year = {2011},
	note = {arXiv: 1012.1086},
	keywords = {Statistics - Computation},
	pages = {1123--1160},
	file = {arXiv\:1012.1086 PDF:files/72/McCoy and Tropp - 2011 - Two Proposals for Robust PCA using Semidefinite Pr.pdf:application/pdf;arXiv.org Snapshot:files/73/1012.html:text/html}
}

@article{shafieezadeh-abadeh_distributionally_2015,
	title = {Distributionally {Robust} {Logistic} {Regression}},
	url = {http://arxiv.org/abs/1509.09259},
	abstract = {This paper proposes a distributionally robust approach to logistic regression. We use the Wasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples. If the radius of this ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence. We then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball. We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases. We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier. These bounds are given by the optimal values of two highly tractable linear programs. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.},
	urldate = {2017-12-23},
	journal = {arXiv:1509.09259 [math, stat]},
	author = {Shafieezadeh-Abadeh, Soroosh and Esfahani, Peyman Mohajerin and Kuhn, Daniel},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.09259},
	keywords = {Statistics - Machine Learning, Mathematics - Optimization and Control},
	annote = {Comment: Neural Information Processing Systems (NIPS), 2015},
	file = {arXiv\:1509.09259 PDF:files/77/Shafieezadeh-Abadeh et al. - 2015 - Distributionally Robust Logistic Regression.pdf:application/pdf;arXiv.org Snapshot:files/78/1509.html:text/html}
}

@article{liu_density_2017,
	title = {Density {Estimation} with {Contaminated} {Data}: {Minimax} {Rates} and {Theory} of {Adaptation}},
	shorttitle = {Density {Estimation} with {Contaminated} {Data}},
	url = {https://128.84.21.199/abs/1712.07801},
	urldate = {2017-12-23},
	author = {Liu, Haoyang and Gao, Chao},
	month = dec,
	year = {2017},
	file = {Full Text PDF:files/80/Liu and Gao - 2017 - Density Estimation with Contaminated Data Minimax.pdf:application/pdf}
}

@article{lecue_robust_2017,
	title = {Robust machine learning by median-of-means : theory and practice},
	shorttitle = {Robust machine learning by median-of-means},
	url = {http://arxiv.org/abs/1711.10306},
	abstract = {We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma{\textasciicircum}2 is sigma{\textasciicircum}2d and it becomes sigma{\textasciicircum}2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation). Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.},
	urldate = {2017-12-23},
	journal = {arXiv:1711.10306 [math, stat]},
	author = {Lecué, Guillaume and Lerasle, Matthieu},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10306},
	keywords = {Mathematics - Statistics Theory},
	annote = {Comment: 48 pages, 6 figures},
	file = {arXiv\:1711.10306 PDF:files/83/Lecué and Lerasle - 2017 - Robust machine learning by median-of-means  theor.pdf:application/pdf;arXiv.org Snapshot:files/84/1711.html:text/html}
}

@article{kothari_outlier-robust_2017,
	title = {Outlier-robust moment-estimation via sum-of-squares},
	url = {http://arxiv.org/abs/1711.11581},
	abstract = {We develop efficient algorithms for estimating low-degree moments of unknown distributions in the presence of adversarial outliers. The guarantees of our algorithms improve in many cases significantly over the best previous ones, obtained in recent works of Diakonikolas et al, Lai et al, and Charikar et al. We also show that the guarantees of our algorithms match information-theoretic lower-bounds for the class of distributions we consider. These improved guarantees allow us to give improved algorithms for independent component analysis and learning mixtures of Gaussians in the presence of outliers. Our algorithms are based on a standard sum-of-squares relaxation of the following conceptually-simple optimization problem: Among all distributions whose moments are bounded in the same way as for the unknown distribution, find the one that is closest in statistical distance to the empirical distribution of the adversarially-corrupted sample.},
	urldate = {2017-12-24},
	journal = {arXiv:1711.11581 [cs, stat]},
	author = {Kothari, Pravesh K. and Steurer, David},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11581},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1711.11581 PDF:files/87/Kothari and Steurer - 2017 - Outlier-robust moment-estimation via sum-of-square.pdf:application/pdf;arXiv.org Snapshot:files/88/1711.html:text/html}
}

@article{huber_robust_1964,
	title = {Robust {Estimation} of a {Location} {Parameter}},
	volume = {35},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177703732},
	doi = {10.1214/aoms/1177703732},
	abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1,⋯,xnx1,⋯,xnx\_1, {\textbackslash}cdots, x\_n be independent random variables with common distribution function F(t−ξ)F(t−ξ)F(t - {\textbackslash}xi). The problem is to estimate the location parameter ξξ{\textbackslash}xi, but with the complication that the prototype distribution F(t)F(t)F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F=(1−ϵ)Φ+ϵHF=(1−ϵ)Φ+ϵHF = (1 - {\textbackslash}epsilon){\textbackslash}Phi + {\textbackslash}epsilon H, where 0≦ϵ{\textless}10≦ϵ{\textless}10 {\textbackslash}leqq {\textbackslash}epsilon {\textless} 1 is a known number, Φ(t)=(2π)−12∫t−∞exp(−12s2)dsΦ(t)=(2π)−12∫−∞texp⁡(−12s2)ds{\textbackslash}Phi(t) = (2{\textbackslash}pi){\textasciicircum}\{-{\textbackslash}frac\{1\}\{2\}\} {\textbackslash}int{\textasciicircum}t\_\{-{\textbackslash}infty\} {\textbackslash}exp(-{\textbackslash}frac\{1\}\{2\}s{\textasciicircum}2) ds is the standard normal cumulative and HHH is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction ϵϵ{\textbackslash}epsilon of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., supt{\textbar}F(t)−Φ(t){\textbar}≦ϵsupt{\textbar}F(t)−Φ(t){\textbar}≦ϵ{\textbackslash}sup\_t {\textbar}F(t) - {\textbackslash}Phi(t){\textbar} {\textbackslash}leqq {\textbackslash}epsilon. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed ϵϵ{\textbackslash}epsilon, there will be several values of ξξ{\textbackslash}xi and σσ{\textbackslash}sigma such that supt{\textbar}F(t)−Φ((t−ξ)/σ){\textbar}≦ϵsupt{\textbar}F(t)−Φ((t−ξ)/σ){\textbar}≦ϵ{\textbackslash}sup\_t{\textbar}F(t) - {\textbackslash}Phi((t - {\textbackslash}xi)/{\textbackslash}sigma){\textbar} {\textbackslash}leqq {\textbackslash}epsilon, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if ϵϵ{\textbackslash}epsilon is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for ξξ{\textbackslash}xi but not for σσ{\textbackslash}sigma); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n→∞n→∞n {\textbackslash}rightarrow {\textbackslash}infty; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of FFF). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression ∑i(xi−T)2∑i(xi−T)2{\textbackslash}sum\_i (x\_i - T){\textasciicircum}2; this is of course achieved by the sample mean T=∑ixi/nT=∑ixi/nT = {\textbackslash}sum\_i x\_i/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T=Tn(x1,⋯,xn)minimizes∑iρ(xi−T),T=Tn(x1,⋯,xn)minimizes∑iρ(xi−T),T = T\_n(x\_1, {\textbackslash}cdots, x\_n) minimizes {\textbackslash}sum\_i {\textbackslash}rho(x\_i - T), whereρisanon−constantfunction.(M)(M)whereρisanon−constantfunction.{\textbackslash}begin\{equation*\} {\textbackslash}tag\{M\} where {\textbackslash}rho is a non-constant function. {\textbackslash}end\{equation*\} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean (ρ(t)=t2)(ρ(t)=t2)({\textbackslash}rho(t) = t{\textasciicircum}2), (ii) the sample median (ρ(t)={\textbar}t{\textbar})(ρ(t)={\textbar}t{\textbar})({\textbackslash}rho(t) = {\textbar}t{\textbar}), and more generally, (iii) all maximum likelihood estimators (ρ(t)=−logf(t)(ρ(t)=−log⁡f(t)({\textbackslash}rho(t) = -{\textbackslash}log f(t), where fff is the assumed density of the untranslated distribution). These (MMM)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x)=Tn(x1,⋯,xn)Tn(x)=Tn(x1,⋯,xn)T\_n(x) = T\_n(x\_1, {\textbackslash}cdots, x\_n)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n→∞)(n→∞)(n {\textbackslash}rightarrow {\textbackslash}infty) when FFF ranges over some suitable set of underlying distributions, in particular over the set of all F=(1−ϵ)Φ+ϵHF=(1−ϵ)Φ+ϵHF = (1 - {\textbackslash}epsilon){\textbackslash}Phi + {\textbackslash}epsilon H for fixed ϵϵ{\textbackslash}epsilon and symmetric HHH. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of nnn it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of HHH, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (MMM)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following ρ:ρ(t)=12t2ρ:ρ(t)=12t2{\textbackslash}rho:{\textbackslash}rho(t) = {\textbackslash}frac\{1\}\{2\}t{\textasciicircum}2 for {\textbar}t{\textbar}{\textless}k,ρ(t)=k{\textbar}t{\textbar}−12k2{\textbar}t{\textbar}{\textless}k,ρ(t)=k{\textbar}t{\textbar}−12k2{\textbar}t{\textbar} {\textless} k, {\textbackslash}rho(t) = k{\textbar}t{\textbar} - {\textbackslash}frac\{1\}\{2\}k{\textasciicircum}2 for {\textbar}t{\textbar}≧k{\textbar}t{\textbar}≧k{\textbar}t{\textbar} {\textbackslash}geqq k, with kkk depending on ϵϵ{\textbackslash}epsilon. This estimator is most robust even among all translation invariant estimators. Sample mean (k=∞)(k=∞)(k = {\textbackslash}infty) and sample median (k=0)(k=0)(k = 0) are limiting cases corresponding to ϵ=0ϵ=0{\textbackslash}epsilon = 0 and ϵ=1ϵ=1{\textbackslash}epsilon = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1≦x2≦⋯≦xnx1≦x2≦⋯≦xnx\_1 {\textbackslash}leqq x\_2 {\textbackslash}leqq {\textbackslash}cdots {\textbackslash}leqq x\_n, then the statistic T=n−1(gxg+1+xg+1+xg+2+⋯+xn−h+hxn−h)T=n−1(gxg+1+xg+1+xg+2+⋯+xn−h+hxn−h)T = n{\textasciicircum}\{-1\}(gx\_\{g + 1\} + x\_\{g + 1\} + x\_\{g + 2\} + {\textbackslash}cdots + x\_\{n - h\} + hx\_\{n - h\}) is called the Winsorized mean, obtained by Winsorizing the ggg leftmost and the hhh rightmost observations. The above most robust (MMM)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg+1xg+1x\_\{g + 1\} and xn−hxn−hx\_\{n - h\} have to be replaced by some numbers u,vu,vu, v satisfying xg≦u≦xg+1xg≦u≦xg+1x\_g {\textbackslash}leqq u {\textbackslash}leqq x\_\{g + 1\} and xn−h≦v≦xn−h+1xn−h≦v≦xn−h+1x\_\{n - h\} {\textbackslash}leqq v {\textbackslash}leqq x\_\{n - h + 1\}, respectively; g,h,ug,h,ug, h, u and vvv depend on the sample. In fact, this (MMM)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0F0F\_0 with density f0(t)=(1−ϵ)(2π)−12e−ρ(t)f0(t)=(1−ϵ)(2π)−12e−ρ(t)f\_0(t) = (1 - {\textbackslash}epsilon)(2{\textbackslash}pi){\textasciicircum}\{-{\textbackslash}frac\{1\}\{2\}\}e{\textasciicircum}\{-{\textbackslash}rho(t)\}. This f0f0f\_0 behaves like a normal density for small ttt, like an exponential density for large ttt. At least for me, this was rather surprising--I would have expected an f0f0f\_0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that FFF belongs to some convex set CCC of distribution functions. Then the most robust (MMM)-estimator for the set CCC coincides with the maximum likelihood estimator for the unique F0εCF0εCF\_0 {\textbackslash}varepsilon C which has the smallest Fisher information number I(F)=∫(f′/f)2fdtI(F)=∫(f′/f)2fdtI(F) = {\textbackslash}int (f'/f){\textasciicircum}2f dt among all FεCFεCF {\textbackslash}varepsilon C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy supt{\textbar}F(t)−Φ(t){\textbar}≦ϵsupt{\textbar}F(t)−Φ(t){\textbar}≦ϵ{\textbackslash}sup\_t{\textbar}F(t) - {\textbackslash}Phi(t){\textbar} {\textbackslash}leqq {\textbackslash}epsilon; robust estimation of a scale parameter; how to estimate location, if scale and ϵϵ{\textbackslash}epsilon are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing ∑i{\textless}jρ(xi−T,xj−T)∑i{\textless}jρ(xi−T,xj−T){\textbackslash}sum\_\{i {\textless} j\} {\textbackslash}rho(x\_i - T, x\_j - T), where ρρ{\textbackslash}rho is a function of two arguments. Questions of small sample size theory will not be touched in this paper.},
	language = {EN},
	number = {1},
	urldate = {2017-12-26},
	journal = {Ann. Math. Statist.},
	author = {Huber, Peter J.},
	month = mar,
	year = {1964},
	mrnumber = {MR161415},
	zmnumber = {0136.39805},
	pages = {73--101},
	file = {euclid.aoms.1177703732.pdf:files/92/euclid.aoms.1177703732.pdf:application/pdf;Snapshot:files/91/1177703732.html:text/html}
}