
@article{cribben_estimating_2015,
	title = {Estimating whole brain dynamics using spectral clustering},
	url = {http://arxiv.org/abs/1509.03730},
	abstract = {The estimation of time-varying networks for functional Magnetic Resonance Imaging (fMRI) data sets is of increasing importance and interest. In this work, we formulate the problem in a high-dimensional time series framework and introduce a data-driven method, namely Network Change Points Detection (NCPD), which detects change points in the network structure of a multivariate time series, with each component of the time series represented by a node in the network. NCPD is applied to various simulated data and a resting-state fMRI data set. This new methodology also allows us to identify common functional states within and across subjects. Finally, NCPD promises to offer a deep insight into the large-scale characterisations and dynamics of the brain},
	urldate = {2018-01-06},
	journal = {arXiv:1509.03730 [stat]},
	author = {Cribben, Ivor and Yu, Yi},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.03730},
	keywords = {Statistics - Methodology, Statistics - Applications},
	annote = {Comment: 24 pages, 9 figures},
	file = {arXiv.org Snapshot:files/97/1509.html:text/html;Cribben and Yu - 2015 - Estimating whole brain dynamics using spectral clu.pdf:files/96/Cribben and Yu - 2015 - Estimating whole brain dynamics using spectral clu.pdf:application/pdf}
}

@article{bing_sparse_2017,
	title = {Sparse {Latent} {Factor} {Models} with {Pure} {Variables} for {Overlapping} {Clustering}},
	url = {http://arxiv.org/abs/1704.06977},
	abstract = {The problem of overlapping variable clustering, ubiquitous in data science, is that of finding overlapping sub-groups of a p-dimensional random vector X, from a sample of size n of observations on X. Typical solutions are algorithmic in nature, and little is known about the statistical guarantees of the estimated clusters, as most algorithms are not model-based. This work introduces a novel method, LOVE, based on a sparse Latent factor model, with correlated factors, and with pure variables, for OVErlapping clustering with statistical guarantees. The model is used to define the population level clusters as groups of those components of X that are associated, via a sparse allocation matrix, with the same unobservable latent factor, and multi-factor association is allowed. Clusters are respectively anchored by components of X, called pure variables, that are associated with only one latent factor. We prove that the existence of pure variables is a sufficient, and almost necessary, assumption for the identifiability of the allocation matrix, in sparse latent factor models. Consequently, model-based clusters can be uniquely defined, and provide a bona fide estimation target. LOVE estimates first the set of pure variables, and the number of clusters, via a novel method that has low computational complexity of order p2. Each cluster, anchored by pure variables, is then further populated with components of X according to the sparse estimates of the allocation matrix. The latter are obtained via a new, computationally efficient, estimation method tailored to the structure of this problem. The combined procedure yields rate-optimal estimates of the allocation matrix and consistent estimators of the number of clusters.},
	urldate = {2018-01-06},
	journal = {arXiv:1704.06977 [math, stat]},
	author = {Bing, Xin and Bunea, Florentina and Ning, Yang and Wegkamp, Marten},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06977},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:files/100/1704.html:text/html;Bing et al. - 2017 - Sparse Latent Factor Models with Pure Variables fo.pdf:files/99/Bing et al. - 2017 - Sparse Latent Factor Models with Pure Variables fo.pdf:application/pdf}
}

@article{bunea_minimax_2015,
	title = {Minimax {Optimal} {Variable} {Clustering} in {G}-{Block} {Correlation} {Models} via {Cord}},
	url = {http://arxiv.org/abs/1508.01939},
	abstract = {The goal of variable clustering is to partition a random vector \$\{{\textbackslash}bf X\} {\textbackslash}in R{\textasciicircum}p\$ in sub-groups of similar probabilistic behavior. Popular methods such as hierarchical clustering or K-means are algorithmic procedures applied to observations on \$\{{\textbackslash}bf X\}\$, while no population-level target is defined prior to estimation. We take a different view in this paper, where we propose and investigate model based variable clustering. We identify variable clusters with a partition G of the variable set, which is the target of estimation. Motivated by the potential lack of identifiability of the G-latent models, which are currently used in problems involving variable clustering, we introduce the class of G-block correlation models and show that they are identifiable. The new class of models allows the unknown number of the clusters K to grow linearly with p, which itself can depend, and be larger, than the sample size. Moreover, the minimum size of a cluster can be as small as 1, and the maximum size can grow as p. In this context, we introduce MCord, a new cluster separation metric, tailored to G-block correlation models. The difficulty of any clustering algorithm is given by the size of the cluster separation required for correct recovery. We derive the minimax lower bound on MCord below which no algorithm can estimate the clusters exactly, and show that its rate is \${\textbackslash}sqrt\{log(p)/n\}\$. We accompany this result by a simple, yet powerful, algorithm, CORD, and show that it recovers exactly the clusters of variables, with high probability, at the minimax optimal MCord separation rate. Our new procedure is available on CRAN and has computational complexity that is polynomial in p. The merits of our model and procedure are illustrated via a data analysis.},
	urldate = {2018-01-06},
	journal = {arXiv:1508.01939 [math, stat]},
	author = {Bunea, Florentina and Giraud, Christophe and Luo, Xi},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.01939},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Machine Learning},
	annote = {Comment: Maintext: 32 pages; supplementary information: 12 pages},
	file = {arXiv.org Snapshot:files/104/1508.html:text/html;Bunea et al. - 2015 - Minimax Optimal Variable Clustering in G-Block Cor.pdf:files/103/Bunea et al. - 2015 - Minimax Optimal Variable Clustering in G-Block Cor.pdf:application/pdf}
}

@article{miwakeichi_decomposing_2004,
	title = {Decomposing {EEG} data into space–time–frequency components using {Parallel} {Factor} {Analysis}},
	volume = {22},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811904001958},
	doi = {10.1016/j.neuroimage.2004.03.039},
	abstract = {Finding the means to efficiently summarize electroencephalographic data has been a long-standing problem in electrophysiology. A popular approach is identification of component modes on the basis of the time-varying spectrum of multichannel EEG recordings—in other words, a space/frequency/time atomic decomposition of the time-varying EEG spectrum. Previous work has been limited to only two of these dimensions. Principal Component Analysis (PCA) and Independent Component Analysis (ICA) have been used to create space/time decompositions; suffering an inherent lack of uniqueness that is overcome only by imposing constraints of orthogonality or independence of atoms. Conventional frequency/time decompositions ignore the spatial aspects of the EEG. Framing of the data being as a three-way array indexed by channel, frequency, and time allows the application of a unique decomposition that is known as Parallel Factor Analysis (PARAFAC). Each atom is the tri-linear decomposition into a spatial, spectral, and temporal signature. We applied this decomposition to the EEG recordings of five subjects during the resting state and during mental arithmetic. Common to all subjects were two atoms with spectral signatures whose peaks were in the theta and alpha range. These signatures were modulated by physiological state, increasing during the resting stage for alpha and during mental arithmetic for theta. Furthermore, we describe a new method (Source Spectra Imaging or SSI) to estimate the location of electric current sources from the EEG spectrum. The topography of the theta atom is frontal and the maximum of the corresponding SSI solution is in the anterior frontal cortex. The topography of the alpha atom is occipital with maximum of the SSI solution in the visual cortex. We show that the proposed decomposition can be used to search for activity with a given spectral and topographic profile in new recordings, and that the method may be useful for artifact recognition and removal.},
	number = {3},
	urldate = {2018-01-06},
	journal = {NeuroImage},
	author = {Miwakeichi, Fumikazu and Martı́nez-Montes, Eduardo and Valdés-Sosa, Pedro A. and Nishiyama, Nobuaki and Mizuhara, Hiroaki and Yamaguchi, Yoko},
	month = jul,
	year = {2004},
	keywords = {EEG space/frequency/time decomposition, Multiway analysis, Parallel Factor Analysis, Principal Component Analysis, Source Spectra Imaging},
	pages = {1035--1045},
	file = {Miwakeichi et al. - 2004 - Decomposing EEG data into space–time–frequency com.pdf:files/105/Miwakeichi et al. - 2004 - Decomposing EEG data into space–time–frequency com.pdf:application/pdf;ScienceDirect Snapshot:files/107/S1053811904001958.html:text/html}
}

@article{yu_gaussian-process_2009,
	title = {Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity},
	volume = {102},
	issn = {0022-3077},
	doi = {10.1152/jn.90941.2008},
	abstract = {We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from many neurons on individual experimental trials. Beyond the benefit of visualizing the high-dimensional, noisy spiking activity in a compact form, such trajectories can offer insight into the dynamics of the neural circuitry underlying the recorded activity. Current methods for extracting neural trajectories involve a two-stage process: the spike trains are first smoothed over time, then a static dimensionality-reduction technique is applied. We first describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way and that account for spiking variability, which may vary both across neurons and across time. We then present a novel method for extracting neural trajectories-Gaussian-process factor analysis (GPFA)-which unifies the smoothing and dimensionality-reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-fit metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that the proposed extensions improved the predictive ability of the two-stage methods. The predictive ability was further improved by going to GPFA. From the extracted trajectories, we directly observed a convergence in neural state during motor planning, an effect that was shown indirectly by previous studies. We then show how such methods can be a powerful tool for relating the spiking activity across a neural population to the subject's behavior on a single-trial basis. Finally, to assess how well the proposed methods characterize neural population activity when the underlying time course is known, we performed simulations that revealed that GPFA performed tens of percent better than the best two-stage method.},
	language = {eng},
	number = {1},
	journal = {J. Neurophysiol.},
	author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
	month = jul,
	year = {2009},
	pmid = {19357332},
	pmcid = {PMC2712272},
	keywords = {Principal Component Analysis, Action Potentials, Animals, Models, Neurological, Nerve Net, Neural Networks (Computer), Neurons, Nonlinear Dynamics, Normal Distribution, Reaction Time, Signal Processing, Computer-Assisted, Time Factors},
	pages = {614--635},
	file = {Yu et al. - 2009 - Gaussian-process factor analysis for low-dimension.pdf:files/110/Yu et al. - 2009 - Gaussian-process factor analysis for low-dimension.pdf:application/pdf}
}

@article{ting_multi-scale_2017,
	title = {Multi-{Scale} {Factor} {Analysis} of {High}-{Dimensional} {Brain} {Signals}},
	url = {http://arxiv.org/abs/1705.06502},
	abstract = {In this paper, we develop an approach to modeling high-dimensional networks with a large number of nodes arranged in a hierarchical and modular structure. We propose a novel multi-scale factor analysis (MSFA) model which partitions the massive spatio-temporal data defined over the complex networks into a finite set of regional clusters. To achieve further dimension reduction, we represent the signals in each cluster by a small number of latent factors. The correlation matrix for all nodes in the network are approximated by lower-dimensional sub-structures derived from the cluster-specific factors. To estimate regional connectivity between numerous nodes (within each cluster), we apply principal components analysis (PCA) to produce factors which are derived as the optimal reconstruction of the observed signals under the squared loss. Then, we estimate global connectivity (between clusters or sub-networks) based on the factors across regions using the RV-coefficient as the cross-dependence measure. This gives a reliable and computationally efficient multi-scale analysis of both regional and global dependencies of the large networks. The proposed novel approach is applied to estimate brain connectivity networks using functional magnetic resonance imaging (fMRI) data. Results on resting-state fMRI reveal interesting modular and hierarchical organization of human brain networks during rest.},
	urldate = {2018-01-06},
	journal = {arXiv:1705.06502 [q-bio, stat]},
	author = {Ting, Chee-Ming and Ombao, Hernando and Salleh, Sh-Hussain},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06502},
	keywords = {Statistics - Applications, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 43 pages},
	file = {arXiv.org Snapshot:files/114/1705.html:text/html;Ting et al. - 2017 - Multi-Scale Factor Analysis of High-Dimensional Br.pdf:files/113/Ting et al. - 2017 - Multi-Scale Factor Analysis of High-Dimensional Br.pdf:application/pdf}
}

@article{lakshmanan_extracting_2015,
	title = {Extracting {Low}-{Dimensional} {Latent} {Structure} from {Time} {Series} in the {Presence} of {Delays}},
	volume = {27},
	issn = {1530-888X},
	doi = {10.1162/NECO_a_00759},
	abstract = {Noisy, high-dimensional time series observations can often be described by a set of low-dimensional latent variables. Commonly used methods to extract these latent variables typically assume instantaneous relationships between the latent and observed variables. In many physical systems, changes in the latent variables manifest as changes in the observed variables after time delays. Techniques that do not account for these delays can recover a larger number of latent variables than are present in the system, thereby making the latent representation more difficult to interpret. In this work, we introduce a novel probabilistic technique, time-delay gaussian-process factor analysis (TD-GPFA), that performs dimensionality reduction in the presence of a different time delay between each pair of latent and observed variables. We demonstrate how using a gaussian process to model the evolution of each latent variable allows us to tractably learn these delays over a continuous domain. Additionally, we show how TD-GPFA combines temporal smoothing and dimensionality reduction into a common probabilistic framework. We present an expectation/conditional maximization either (ECME) algorithm to learn the model parameters. Our simulations demonstrate that when time delays are present, TD-GPFA is able to correctly identify these delays and recover the latent space. We then applied TD-GPFA to the activity of tens of neurons recorded simultaneously in the macaque motor cortex during a reaching task. TD-GPFA is able to better describe the neural activity using a more parsimonious latent space than GPFA, a method that has been used to interpret motor cortex data but does not account for time delays. More broadly, TD-GPFA can help to unravel the mechanisms underlying high-dimensional time series data by taking into account physical delays in the system.},
	language = {eng},
	number = {9},
	journal = {Neural Comput},
	author = {Lakshmanan, Karthik C. and Sadtler, Patrick T. and Tyler-Kabara, Elizabeth C. and Batista, Aaron P. and Yu, Byron M.},
	month = sep,
	year = {2015},
	pmid = {26079746},
	pmcid = {PMC4545403},
	keywords = {Action Potentials, Animals, Models, Neurological, Neurons, Normal Distribution, Time Factors, Algorithms, Computer Simulation, Macaca, Motor Cortex, Probability},
	pages = {1825--1856}
}

@article{cichocki_tensor_2013,
	title = {Tensor {Decompositions}: {A} {New} {Concept} in {Brain} {Data} {Analysis}?},
	shorttitle = {Tensor {Decompositions}},
	url = {http://arxiv.org/abs/1305.0395},
	abstract = {Matrix factorizations and their extensions to tensor factorizations and decompositions have become prominent techniques for linear and multilinear blind source separation (BSS), especially multiway Independent Component Analysis (ICA), NonnegativeMatrix and Tensor Factorization (NMF/NTF), Smooth Component Analysis (SmoCA) and Sparse Component Analysis (SCA). Moreover, tensor decompositions have many other potential applications beyond multilinear BSS, especially feature extraction, classification, dimensionality reduction and multiway clustering. In this paper, we briefly overview new and emerging models and approaches for tensor decompositions in applications to group and linked multiway BSS/ICA, feature extraction, classification andMultiway Partial Least Squares (MPLS) regression problems. Keywords: Multilinear BSS, linked multiway BSS/ICA, tensor factorizations and decompositions, constrained Tucker and CP models, Penalized Tensor Decompositions (PTD), feature extraction, classification, multiway PLS and CCA.},
	urldate = {2018-01-06},
	journal = {arXiv:1305.0395 [cs, q-bio, stat]},
	author = {Cichocki, Andrzej},
	month = may,
	year = {2013},
	note = {arXiv: 1305.0395},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition, Computer Science - Numerical Analysis},
	file = {arXiv.org Snapshot:files/119/1305.html:text/html;Cichocki - 2013 - Tensor Decompositions A New Concept in Brain Data.pdf:files/118/Cichocki - 2013 - Tensor Decompositions A New Concept in Brain Data.pdf:application/pdf}
}

@article{barigozzi_simultaneous_2016,
	title = {Simultaneous multiple change-point and factor analysis for high-dimensional time series},
	url = {http://arxiv.org/abs/1612.06928},
	abstract = {We propose the first comprehensive treatment of high-dimensional time series factor models with multiple change-points in their second-order structure. We operate under the most flexible definition of piecewise stationarity, and estimate the number and locations of change-points consistently as well as identifying whether they originate in the common or idiosyncratic components. Through the use of wavelets, we transform the problem of change-point detection in the second-order structure of a high-dimensional time series, into the (relatively easier) problem of change-point detection in the means of high-dimensional panel data. Our methodology circumvents the difficult issue of the accurate estimation of the true number of factors by adopting a screening procedure. In extensive simulation studies, we show that factor analysis prior to change-point detection improves the detectability of change-points, and identify and describe an interesting 'spillover' effect in which substantial breaks in the idiosyncratic components get, naturally enough, identified as change-points in the common components, which prompts us to regard the corresponding change-points as also acting as a form of 'factors'. We introduce a simple graphical tool for visualising the piecewise stationary evolution of the factor structure over time. Our methodology is implemented in the R package factorcpt, available from CRAN.},
	urldate = {2018-01-06},
	journal = {arXiv:1612.06928 [stat]},
	author = {Barigozzi, Matteo and Cho, Haeran and Fryzlewicz, Piotr},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.06928},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:files/122/1612.html:text/html;Barigozzi et al. - 2016 - Simultaneous multiple change-point and factor anal.pdf:files/121/Barigozzi et al. - 2016 - Simultaneous multiple change-point and factor anal.pdf:application/pdf}
}

@article{betzel_multi-scale_2016,
	title = {Multi-scale brain networks},
	url = {http://arxiv.org/abs/1608.08828},
	abstract = {The network architecture of the human brain has become a feature of increasing interest to the neuroscientific community, largely because of its potential to illuminate human cognition, its variation over development and aging, and its alteration in disease or injury. Traditional tools and approaches to study this architecture have largely focused on single scales -- of topology, time, and space. Expanding beyond this narrow view, we focus this review on pertinent questions and novel methodological advances for the multi-scale brain. We separate our exposition into content related to multi-scale topological structure, multi-scale temporal structure, and multi-scale spatial structure. In each case, we recount empirical evidence for such structures, survey network-based methodological approaches to reveal these structures, and outline current frontiers and open questions. Although predominantly peppered with examples from human neuroimaging, we hope that this account will offer an accessible guide to any neuroscientist aiming to measure, characterize, and understand the full richness of the brain's multiscale network structure -- irrespective of species, imaging modality, or spatial resolution.},
	urldate = {2018-01-06},
	journal = {arXiv:1608.08828 [physics, q-bio]},
	author = {Betzel, Richard F. and Bassett, Danielle S.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.08828},
	keywords = {Quantitative Biology - Neurons and Cognition, Physics - Physics and Society},
	annote = {Comment: 12 pages, 3 figures, review article},
	file = {arXiv.org Snapshot:files/126/1608.html:text/html;Betzel and Bassett - 2016 - Multi-scale brain networks.pdf:files/125/Betzel and Bassett - 2016 - Multi-scale brain networks.pdf:application/pdf}
}

@article{park_estimating_2014,
	title = {Estimating {Time}-{Evolving} {Partial} {Coherence} {Between} {Signals} via {Multivariate} {Locally} {Stationary} {Wavelet} {Processes}},
	volume = {62},
	issn = {1053-587X},
	doi = {10.1109/TSP.2014.2343937},
	abstract = {We consider the problem of estimating time-localized cross-dependence in a collection of nonstationary signals. To this end, we develop the multivariate locally stationary wavelet framework, which provides a time-scale decomposition of the signals and, thus, naturally captures the time-evolving scale-specific cross-dependence between components of the signals. Under the proposed model, we rigorously define and estimate two forms of cross-dependence measures: wavelet coherence and wavelet partial coherence. These dependence measures differ in a subtle but important way. The former is a broad measure of dependence, which may include indirect associations, i.e., dependence between a pair of signals that is driven by another signal. Conversely, wavelet partial coherence measures direct linear association between a pair of signals, i.e., it removes the linear effect of other observed signals. Our time-scale wavelet partial coherence estimation scheme thus provides a mechanism for identifying hidden dynamic relationships within a network of nonstationary signals, as we demonstrate on electroencephalograms recorded in a visual-motor experiment.},
	number = {20},
	journal = {IEEE Transactions on Signal Processing},
	author = {Park, T. and Eckley, I. A. and Ombao, H. C.},
	month = oct,
	year = {2014},
	keywords = {Brain modeling, coherence, Coherence, Correlation, Covariance matrices, decomposition, dependence measures, direct linear association, Discrete wavelet transforms, electroencephalograms, electroencephalography, estimation theory, evolutionary computation, local stationarity, multivariate locally stationary wavelet processes, multivariate signals, nonstationary signals network, partial coherence, signal components, signal processing, time-evolving partial coherence estimation, time-evolving scale-specific cross-dependence, time-localized cross-dependence estimation, time-scale decomposition, Transfer functions, Vectors, visual-motor experiment, wavelet coherence, wavelet partial coherence, wavelet transforms, wavelets},
	pages = {5240--5250},
	file = {IEEE Xplore Abstract Record:files/128/6868283.html:text/html}
}

@article{kolar_estimating_2010,
	title = {Estimating time-varying networks},
	volume = {4},
	issn = {1932-6157},
	url = {http://arxiv.org/abs/0812.5087},
	doi = {10.1214/09-AOAS308},
	abstract = {Stochastic networks are a plausible representation of the relational information among entities in dynamic systems such as living cells or social communities. While there is a rich literature in estimating a static or temporally invariant network from observation data, little has been done toward estimating time-varying networks from time series of entity attributes. In this paper we present two new machine learning methods for estimating time-varying networks, which both build on a temporally smoothed \$l\_1\$-regularized logistic regression formalism that can be cast as a standard convex-optimization problem and solved efficiently using generic solvers scalable to large networks. We report promising results on recovering simulated time-varying networks. For real data sets, we reverse engineer the latent sequence of temporally rewiring political networks between Senators from the US Senate voting records and the latent evolving regulatory networks underlying 588 genes across the life cycle of Drosophila melanogaster from the microarray time course.},
	number = {1},
	urldate = {2018-01-06},
	journal = {The Annals of Applied Statistics},
	author = {Kolar, Mladen and Song, Le and Ahmed, Amr and Xing, Eric P.},
	month = mar,
	year = {2010},
	note = {arXiv: 0812.5087},
	keywords = {Statistics - Methodology, Statistics - Machine Learning, Statistics - Applications, Quantitative Biology - Molecular Networks, Quantitative Biology - Quantitative Methods},
	pages = {94--123},
	annote = {Comment: Published in at http://dx.doi.org/10.1214/09-AOAS308 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org)},
	file = {arXiv.org Snapshot:files/132/0812.html:text/html;Kolar et al. - 2010 - Estimating time-varying networks.pdf:files/131/Kolar et al. - 2010 - Estimating time-varying networks.pdf:application/pdf}
}

@article{qiao_functional_2017,
	title = {Functional {Graphical} {Models}},
	issn = {0162-1459},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.2017.1390466},
	doi = {10.1080/01621459.2017.1390466},
	abstract = {Graphical models have attracted increasing attention in recent years, especially in settings involving high dimensional data. In particular Gaussian graphical models are used to model the conditional dependence structure among multiple Gaussian random variables. As a result of its computational efficiency the graphical lasso (glasso) has become one of the most popular approaches for fitting high dimensional graphical models. In this article we extend the graphical models concept to model the conditional dependence structure among p random functions. In this setting, not only is p large, but each function is itself a high dimensional object, posing an additional level of statistical and computational complexity. We develop an extension of the glasso criterion (fglasso), which estimates the functional graphical model by imposing a block sparsity constraint on the precision matrix, via a group lasso penalty. The fglasso criterion can be optimized using an efficient block coordinate descent algorithm. We establish the concentration inequalities of the estimates, which guarantee the desirable graph support recovery property, i.e. with probability tending to one, the fglasso will correctly identify the true conditional dependence structure. Finally we show that the fglasso significantly outperforms possible competing methods through both simulations and an analysis of a real world EEG data set comparing alcoholic and non-alcoholic patients.},
	urldate = {2018-01-06},
	journal = {Journal of the American Statistical Association},
	author = {Qiao, Xinghao and Guo, Shaojun and James, Gareth M.},
	month = oct,
	year = {2017},
	pages = {0--0},
	file = {Qiao et al. - 2017 - Functional Graphical Models.pdf:files/133/Qiao et al. - 2017 - Functional Graphical Models.pdf:application/pdf;Snapshot:files/135/01621459.2017.html:text/html}
}

@article{zhou_time_2008,
	title = {Time {Varying} {Undirected} {Graphs}},
	url = {http://arxiv.org/abs/0802.2758},
	abstract = {Undirected graphs are often used to describe high dimensional distributions. Under sparsity conditions, the graph can be estimated using \${\textbackslash}ell\_1\$ penalization methods. However, current methods assume that the data are independent and identically distributed. If the distribution, and hence the graph, evolves over time then the data are not longer identically distributed. In this paper, we show how to estimate the sequence of graphs for non-identically distributed data, where the distribution evolves over time.},
	urldate = {2018-01-06},
	journal = {arXiv:0802.2758 [math, stat]},
	author = {Zhou, Shuheng and Lafferty, John and Wasserman, Larry},
	month = feb,
	year = {2008},
	note = {arXiv: 0802.2758},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
	annote = {Comment: 12 pages, 3 figures, to appear in COLT 2008},
	file = {arXiv.org Snapshot:files/141/0802.html:text/html;Time-Varying-Undirected-Graphs-Zhou-2010.pdf:files/136/Time-Varying-Undirected-Graphs-Zhou-2010.pdf:application/pdf}
}

@misc{noauthor_brain_nodate,
	title = {Brain {Oscillations} and the {Importance} of {Waveform} {Shape}: {Trends} in {Cognitive} {Sciences}},
	url = {http://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(16)30218-2},
	urldate = {2018-01-06},
	file = {Brain Oscillations and the Importance of Waveform Shape\: Trends in Cognitive Sciences:files/144/S1364-6613(16)30218-2.html:text/html}
}

@article{buzsaki_origin_2012,
	title = {The origin of extracellular fields and currents — {EEG}, {ECoG}, {LFP} and spikes},
	volume = {13},
	copyright = {2012 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3241},
	doi = {10.1038/nrn3241},
	abstract = {{\textless}p{\textgreater}
                {\textless}list list-type="bullet"{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}All currents in the brain superimpose to yield an 'electric field' at any given point in space. The current sources and sinks form dipoles or higher-order n-poles.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}Extracellular currents arise from many sources, including synaptic currents, fast action potentials and their afterpotentials, calcium spikes and voltage-dependent intrinsic currents.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}The magnitude of extracellular currents depends critically on two factors: the cytoarchitectural organization of a network and the temporal synchrony of the various current sinks and sources.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}Depending on the recording method, neuroscientists distinguish between electroencephalogram (EEG), electrocorticogram (ECoG) and local field potential (LFP; also known as micro-, depth or intracranial EEG), although all of these measures refer to the same biophysical process.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}The electric field is the force 'felt' by an electric charge, and can be transmitted through brain volume. The extent of volume conduction depends on the relationships between the current dipole and the features of the conductive medium.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}High-density sampling of the extracellular field with contemporary methods enables the calculation of current source density, and therefore the localization of current sinks and sources.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}The voltage gradients generated by highly synchronous activity of neuronal groups can affect the transmembrane potential of the member neurons and alter their excitability through ephaptic coupling.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}Synchronous spiking of nearby neurons is the main source of the high-frequency components of the local field.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                    {\textless}li{\textgreater}{\textless}p{\textgreater}There is a discernable relationship between the temporal evolution of cell assemblies and the time-dependent changes of the spatially distributed currents. High-density, wide-band recordings of the local field can therefore provide access to both afferent inputs and the spiking output of neurons.{\textless}/p{\textgreater}{\textless}/li{\textgreater}
                {\textless}/ul{\textgreater}
            {\textless}/p{\textgreater}},
	language = {En},
	number = {6},
	urldate = {2018-01-06},
	journal = {Nature Reviews Neuroscience},
	author = {Buzsáki, György and Anastassiou, Costas A. and Koch, Christof},
	month = jun,
	year = {2012},
	pages = {407},
	file = {Snapshot:files/146/nrn3241.html:text/html}
}

@article{fiecas_modeling_2016,
	title = {Modeling the {Evolution} of {Dynamic} {Brain} {Processes} {During} an {Associative} {Learning} {Experiment}},
	volume = {111},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1165683},
	doi = {10.1080/01621459.2016.1165683},
	abstract = {We develop a new time series model to investigate the dynamic interactions between the nucleus accumbens and the hippocampus during an associative learning experiment. Preliminary analyses indicated that the spectral properties of the local field potentials at these two regions changed over the trials of the experiment. While many models already take into account nonstationarity within a single trial, the evolution of the dynamics across trials is often ignored. Our proposed model, the slowly evolving locally stationary process (SEv-LSP), is designed to capture nonstationarity both within a trial and across trials. We rigorously define the evolving evolutionary spectral density matrix, which we estimate using a two-stage procedure. In the first stage, we compute the within-trial time-localized periodogram matrix. In the second stage, we develop a data-driven approach that combines information from trial-specific local periodogram matrices. Through simulation studies, we show the utility of our proposed method for analyzing time series data with different evolutionary structures. Finally, we use the SEv-LSP model to demonstrate the evolving dynamics between the hippocampus and the nucleus accumbens during an associative learning experiment. Supplementary materials for this article are available online.},
	number = {516},
	urldate = {2018-01-06},
	journal = {Journal of the American Statistical Association},
	author = {Fiecas, Mark and Ombao, Hernando},
	month = oct,
	year = {2016},
	keywords = {coherence, Bivariate time series, Local stationarity, Replicated time series, Signal heterogeneity, Spectral analysis},
	pages = {1440--1453},
	file = {Snapshot:files/148/01621459.2016.html:text/html}
}

@article{cole_nonsinusoidal_2017,
	title = {Nonsinusoidal {Beta} {Oscillations} {Reflect} {Cortical} {Pathophysiology} in {Parkinson}'s {Disease}},
	volume = {37},
	issn = {1529-2401},
	doi = {10.1523/JNEUROSCI.2208-16.2017},
	abstract = {Oscillations in neural activity play a critical role in neural computation and communication. There is intriguing new evidence that the nonsinusoidal features of the oscillatory waveforms may inform underlying physiological and pathophysiological characteristics. Time-domain waveform analysis approaches stand in contrast to traditional Fourier-based methods, which alter or destroy subtle waveform features. Recently, it has been shown that the waveform features of oscillatory beta (13-30 Hz) events, a prominent motor cortical oscillation, may reflect near-synchronous excitatory synaptic inputs onto cortical pyramidal neurons. Here we analyze data from invasive human primary motor cortex (M1) recordings from patients with Parkinson's disease (PD) implanted with a deep brain stimulator (DBS) to test the hypothesis that the beta waveform becomes less sharp with DBS, suggesting that M1 input synchrony may be decreased. We find that, in PD, M1 beta oscillations have sharp, asymmetric, nonsinusoidal features, specifically asymmetries in the ratio between the sharpness of the beta peaks compared with the troughs. This waveform feature is nearly perfectly correlated with beta-high gamma phase-amplitude coupling (r = 0.94), a neural index previously shown to track PD-related motor deficit. Our results suggest that the pathophysiological beta generator is altered by DBS, smoothing out the beta waveform. This has implications not only for the interpretation of the physiological mechanism by which DBS reduces PD-related motor symptoms, but more broadly for our analytic toolkit in general. That is, the often-overlooked time-domain features of oscillatory waveforms may carry critical physiological information about neural processes and dynamics.SIGNIFICANCE STATEMENT To better understand the neural basis of cognition and disease, we need to understand how groups of neurons interact to communicate with one another. For example, there is evidence that parkinsonian bradykinesia and rigidity may arise from an oversynchronization of afferents to the motor cortex, and that these symptoms are treatable using deep brain stimulation. Here we show that the waveform shape of beta (13-30 Hz) oscillations, which may reflect input synchrony onto the cortex, is altered by deep brain stimulation. This suggests that mechanistic inferences regarding physiological and pathophysiological neural communication may be made from the temporal dynamics of oscillatory waveform shape.},
	language = {eng},
	number = {18},
	journal = {J. Neurosci.},
	author = {Cole, Scott R. and van der Meij, Roemer and Peterson, Erik J. and de Hemptinne, Coralie and Starr, Philip A. and Voytek, Bradley},
	month = may,
	year = {2017},
	pmid = {28416595},
	pmcid = {PMC5426572},
	keywords = {Models, Neurological, Nerve Net, Computer Simulation, Motor Cortex, Aged, beta, Beta Rhythm, Biological Clocks, Brain Mapping, Cortical Synchronization, Female, Humans, Male, Middle Aged, motor cortex, oscillation, Parkinson Disease, Parkinson's disease, phase-amplitude coupling, waveform},
	pages = {4830--4840}
}

@article{yu_statistical_2017,
	title = {Statistical {Challenges} in {Modeling} {Big} {Brain} {Signals}},
	url = {http://arxiv.org/abs/1711.00432},
	abstract = {Brain signal data are inherently big: massive in amount, complex in structure, and high in dimensions. These characteristics impose great challenges for statistical inference and learning. Here we review several key challenges, discuss possible solutions, and highlight future research directions.},
	urldate = {2018-01-06},
	journal = {arXiv:1711.00432 [stat]},
	author = {Yu, Zhaoxia and Pluta, Dustin and Shen, Tong and Chen, Chuansheng and Xue, Gui and Ombao, Hernando},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.00432},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:files/153/1711.html:text/html;Yu et al. - 2017 - Statistical Challenges in Modeling Big Brain Signa.pdf:files/152/Yu et al. - 2017 - Statistical Challenges in Modeling Big Brain Signa.pdf:application/pdf}
}